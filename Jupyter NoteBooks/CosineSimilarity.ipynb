{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe9df8f9i8fv",
        "outputId": "561efada-62f4-4e97-cc0a-080690db750b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuLG8qtLjAcj",
        "outputId": "d4bcfb77-6f36-4215-a049-f80605b7f764"
      },
      "outputs": [],
      "source": [
        "%cd '/content/drive/MyDrive/Final_Model/stylegan2-swapper'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wubYCU1UL7d_",
        "outputId": "ae44e63a-a5dd-4dba-cfb3-e8cef82aaa75"
      },
      "outputs": [],
      "source": [
        "CODE_DIR = 'StyleID'\n",
        "\n",
        "!git clone https://github.com/minha12/StyleID.git\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "\n",
        "# os.chdir(f'./{CODE_DIR}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1cGPlRlMVdX",
        "outputId": "698a0096-e582-48e8-aefa-5986269b7de4"
      },
      "outputs": [],
      "source": [
        "!pip install mtcnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "JkdlpmmdMZLA",
        "outputId": "4de2cc5e-12a5-42d2-bc67-d8786f7b5e83"
      },
      "outputs": [],
      "source": [
        "from StyleID.mtcnn import mtcnn\n",
        "import numpy as np\n",
        "\n",
        "def detect_face(img):\n",
        "  # Convert PIL Image to NumPy array\n",
        "  img_array = np.asarray(img)\n",
        "  # Step 1: Instantiate MTCNN\n",
        "  face_recongniser = MTCNN()\n",
        "  # Step 3: Face Detection\n",
        "  faces = face_recongniser.detect_faces(img_array)\n",
        "  return faces\n",
        "\n",
        "  /content/drive/MyDrive/Final_Model/stylegan2-swapper/StyleID/models/mtcnn/mtcnn.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMjfl8PuieQb"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "import requests\n",
        "import bz2\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import dlib\n",
        "from pix2pixHD.data.base_dataset import __scale_width\n",
        "from pix2pixHD.models.networks import define_G\n",
        "import pix2pixHD.util.util as util\n",
        "from aligner import align_face\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from google.colab import output\n",
        "from IPython.display import HTML\n",
        "import base64\n",
        "import imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgjGmU4piXB4"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import IPython.display as display\n",
        "import io\n",
        "\n",
        "img_url = '/content/drive/MyDrive/Final_Model/Test_F2.jpg'\n",
        "file_name = 'input_image.jpg'\n",
        "# Open the image using PIL\n",
        "with open(img_url, 'rb') as f:\n",
        "    img = Image.open(io.BytesIO(f.read()))\n",
        "# display.display(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbJHVh3-MUgq",
        "outputId": "c75908e9-98c8-4b4f-cef1-fbe1cf5af781"
      },
      "outputs": [],
      "source": [
        "faces = detect_face(img)\n",
        "if faces is not None:\n",
        "  imageio.v2.imwrite(file_name, imageio.v2.imread(img_url))\n",
        "  # print(\"Found Faces\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JTs8_8Oj21m"
      },
      "source": [
        "**Face Alignment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0G4VEgZj187"
      },
      "outputs": [],
      "source": [
        "def unpack_bz2(src_path):\n",
        "    data = bz2.BZ2File(src_path).read()\n",
        "    dst_path = src_path[:-4]\n",
        "    with open(dst_path, 'wb') as fp:\n",
        "        fp.write(data)\n",
        "    return dst_path\n",
        "\n",
        "def download(url, file_name):\n",
        "    with open(file_name, \"wb\") as file:\n",
        "        response = requests.get(url)\n",
        "        file.write(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBYLKQJ1jzN1"
      },
      "outputs": [],
      "source": [
        "# Pre-trained model for facial landmark detection\n",
        "\n",
        "import bz2\n",
        "import shutil\n",
        "\n",
        "# Define the paths\n",
        "shape_model_drive_path = '/content/drive/MyDrive/Final_Model/shape_predictor_68_face_landmarks.dat.bz2'\n",
        "shape_model_extracted_path = '/content/drive/MyDrive/Final_Model/shape_predictor_68_face_landmarks.dat'\n",
        "\n",
        "# Extract the contents in Google Drive\n",
        "with open(shape_model_extracted_path, 'wb') as new_file, bz2.BZ2File(shape_model_drive_path, 'rb') as file:\n",
        "    shutil.copyfileobj(file, new_file)\n",
        "\n",
        "# Load the model\n",
        "shape_predictor = dlib.shape_predictor(shape_model_extracted_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U92fQoFpkJM3",
        "outputId": "cd744da8-f733-478e-9b6e-463f0f75c0b5"
      },
      "outputs": [],
      "source": [
        "# aligned_img = align_face(file_name, shape_predictor)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttHvT583kP9X"
      },
      "source": [
        "**Prepare pix2pixHD model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO5lw8RFkMrA"
      },
      "outputs": [],
      "source": [
        "def get_eval_transform(loadSize=512):\n",
        "    transform_list = []\n",
        "    transform_list.append(transforms.Lambda(lambda img: __scale_width(img,\n",
        "                                                                      loadSize,\n",
        "                                                                      Image.BICUBIC)))\n",
        "    transform_list += [transforms.ToTensor()]\n",
        "    transform_list += [transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                                            (0.5, 0.5, 0.5))]\n",
        "    return transforms.Compose(transform_list)\n",
        "\n",
        "transform = get_eval_transform()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dl6vFm54kZ76",
        "outputId": "3624ecd2-d5c2-46cf-a16e-031d3e48f6a2"
      },
      "outputs": [],
      "source": [
        "# To male:\n",
        "!gdown --id 1-6J1CYLsIysk38X9DNN23lIcnvOr8aYh # https://drive.google.com/file/d/1-6J1CYLsIysk38X9DNN23lIcnvOr8aYh/view?usp=sharing\n",
        "\n",
        "# To female:\n",
        "!gdown --id 1frJERJr0WM_R38LnSFQ6XjGQtcXnLco1 # https://drive.google.com/file/d/1frJERJr0WM_R38LnSFQ6XjGQtcXnLco1/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i9uGMsVkdv0",
        "outputId": "66f8544c-5536-4099-e43c-342da5327ae9"
      },
      "outputs": [],
      "source": [
        "config_G = {\n",
        "    'input_nc': 3,\n",
        "    'output_nc': 3,\n",
        "    'ngf': 64,\n",
        "    'netG': 'global',\n",
        "    'n_downsample_global': 4,\n",
        "    'n_blocks_global': 9,\n",
        "    'n_local_enhancers': 1,\n",
        "    'norm': 'instance',\n",
        "}\n",
        "\n",
        "weights_path = '/content/drive/MyDrive/Final_Model/to_male_net_G.pth'  # to_female_net_G.pth\n",
        "\n",
        "model = define_G(**config_G)\n",
        "pretrained_dict = torch.load(weights_path)\n",
        "model.load_state_dict(pretrained_dict)\n",
        "model.cuda();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3botJy1kxf6"
      },
      "source": [
        "**Use Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lba1tzJOk2iS"
      },
      "outputs": [],
      "source": [
        "img = transform(aligned_img).unsqueeze(0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model(img.cuda())\n",
        "\n",
        "out = util.tensor2im(out.data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "C2jciNekk5ii",
        "outputId": "bc6f90f2-af18-40ff-d662-ab3083a1a9b7"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(aligned_img)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(out)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlNiz_O5lGur"
      },
      "source": [
        "**Save the output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwcbCqjdk-ef"
      },
      "outputs": [],
      "source": [
        "imageio.imsave('result.jpg', out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7SEVwiclSVa",
        "outputId": "cca601a5-666f-43b1-e016-78f57c08766e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torchvision import models\n",
        "\n",
        "# Load the pre-trained ResNet-18 model\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "# Freeze the model parameters\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Modify the last fully connected layer for gender classification\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, 2)  # 2 classes: male and female\n",
        "\n",
        "# Load the weights from a checkpoint\n",
        "checkpoint_path = '/content/drive/MyDrive/Final_Model/face_gender_classification_transfer_learning_with_ResNet18.pth'\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "model.load_state_dict(checkpoint)\n",
        "\n",
        "\n",
        "# Load the image and perform transformations\n",
        "def load_image(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    return preprocess(image).unsqueeze(0)\n",
        "\n",
        "# Define class labels\n",
        "class_labels = {0: 'Male', 1: 'Female'}\n",
        "\n",
        "# Function to predict gender\n",
        "def predict_gender(image_path):\n",
        "    model.eval()\n",
        "    image_tensor = load_image(image_path)\n",
        "    outputs = model(image_tensor)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    predicted_label = class_labels[predicted.item()]\n",
        "    return predicted_label\n",
        "\n",
        "# Example usage: path to the image you want to classify\n",
        "result = predict_gender(img_url)\n",
        "print(f\"Predicted gender: {result}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuh4omr9lfmf",
        "outputId": "ea2b4c1d-5a0f-4e27-9cdb-2aac04c74fd1"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load(checkpoint_path)\n",
        "print(checkpoint.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4YCQ3Wgli12",
        "outputId": "9c1d9382-fd82-4372-cef3-84c4a0133716"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Convert the PyTorch tensor to a NumPy array\n",
        "img_np = img.squeeze().permute(1, 2, 0).cpu().numpy()\n",
        "# print(img_np.shape)\n",
        "# Reshape the NumPy array to match the shape of the PyTorch tensor\n",
        "img_np_flat = img_np.flatten()\n",
        "# print(img_np_flat.shape)\n",
        "# Reshape the NumPy array to a 1D array\n",
        "out_flat = out.flatten()\n",
        "# print(out_flat.shape)\n",
        "# Reshape the NumPy array to a 2D array (1 sample, -1 features)\n",
        "img_np_flat = img_np_flat.reshape(1, -1)\n",
        "out_flat = out_flat.reshape(1, -1)\n",
        "# print(img_np_flat.shape)\n",
        "# print(out_flat.shape)\n",
        "\n",
        "# Calculate cosine similarity using sklearn\n",
        "cosine_sim = cosine_similarity(img_np_flat, out_flat)\n",
        "\n",
        "# Print the cosine similarity\n",
        "print(\"Cosine Similarity:\", cosine_sim[0, 0])\n",
        "\n",
        "# Convert cosine similarity to percentage\n",
        "percentage_similarity = ((cosine_sim[0, 0] + 1) / 2) * 100\n",
        "\n",
        "# Print the percentage similarity\n",
        "print(f\"Similarity Percentage: {percentage_similarity:.2f}%\")\n",
        "# print(img.shape)\n",
        "# print(out.shape)\n",
        "\n",
        "# input_tensor = img.to(device)\n",
        "# output_tensor = out.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J92Ttch9p_fM",
        "outputId": "a5e5e58f-2859-4eaf-aa7e-109cb77f569f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Convert the NumPy array to a PyTorch tensor\n",
        "output_tensor = torch.from_numpy(out).permute(2, 0, 1).unsqueeze(0).float()\n",
        "\n",
        "# Print the shape of the resulting tensor\n",
        "# print(\"Tensor Shape:\", tensor.shape)\n",
        "# print(len(tensor))\n",
        "# Flatten the tensors to 1D vectors (if needed)\n",
        "tensor1_flat = img.view(1, -1)\n",
        "tensor2_flat = output_tensor.view(1, -1)\n",
        "\n",
        "\n",
        "# define a method to measure cosine similarity\n",
        "cos = torch.nn.CosineSimilarity(dim=1)\n",
        "output = cos(tensor1_flat, tensor2_flat)\n",
        "\n",
        "# display the output tensor\n",
        "print(\"Cosine Similarity:\",output)\n",
        "\n",
        "# Assuming 'output' contains the cosine similarity value\n",
        "percentage_similarity = ((output.item() + 1) / 2) * 100\n",
        "print(f\"Similarity Percentage: {percentage_similarity:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFY9Lw5eqf0A",
        "outputId": "e2ca8e41-a7a8-43b3-a44c-3032cb3c761b"
      },
      "outputs": [],
      "source": [
        "# using torch\n",
        "def find_cosine_similarity(input_tensor, generated_image):\n",
        "  # Convert the NumPy array to a PyTorch tensor\n",
        "  output_tensor = torch.from_numpy(generated_image).permute(2, 0, 1).unsqueeze(0).float()\n",
        "  # Flatten the tensors to 1D vectors (if needed)\n",
        "  tensor1_flat = input_tensor.view(1, -1)\n",
        "  tensor2_flat = output_tensor.view(1, -1)\n",
        "\n",
        "  # define a method to measure cosine similarity\n",
        "  cos = torch.nn.CosineSimilarity(dim=1)\n",
        "  output = cos(tensor1_flat, tensor2_flat)\n",
        "\n",
        "  # display the output tensor\n",
        "  return output\n",
        "\n",
        "def get_similarity_percentage_using_torch(input_tensor, generated_image):\n",
        "  cos = find_cosine_similarity(input_tensor, generated_image)\n",
        "  percentage= ((cos.item() + 1) / 2) * 100\n",
        "  return percentage\n",
        "\n",
        "print(find_cosine_similarity(img, out))\n",
        "print(get_similarity_percentage_using_torch(img, out))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "hk_pXJrUxGcv",
        "outputId": "dd19e7d0-1665-44b0-c3e7-57f6c52c4109"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Path to your dataset folder in Google Drive\n",
        "dataset_folder = '/content/drive/MyDrive/Final_Model/Normal_Image_Testing'\n",
        "\n",
        "sum_of_percentage = 0\n",
        "num_of_files = 0\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "# Iterate through all files in the folder\n",
        "for filename in os.listdir(dataset_folder):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):  # Check if it's an image file\n",
        "        # Load the image using PIL from the dataset folder\n",
        "        aligned_img = align_face(os.path.join(dataset_folder, filename), shape_predictor)[0]\n",
        "        img = transform(aligned_img).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            output = model(img.cuda())\n",
        "        out = util.tensor2im(output.data[0])\n",
        "\n",
        "        sum_of_percentage += get_similarity_percentage_using_torch(img, out)\n",
        "        num_of_files += 1\n",
        "\n",
        "# Stop the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the time taken\n",
        "elapsed_time = end_time - start_time\n",
        "elapsed_minutes = int(elapsed_time_seconds // 60)\n",
        "elapsed_seconds = int(elapsed_time_seconds % 60)\n",
        "\n",
        "average = sum_of_percentage / num_of_files\n",
        "print(f\"Average Similarity Percentage: {average:.2f}%\")\n",
        "print(f\"Time taken: {elapsed_minutes} minutes and {elapsed_seconds} seconds of {num_of_files} files\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2ZTPpkCTg0a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Path to your dataset folder in Google Drive\n",
        "dataset_folder = '/content/drive/MyDrive/Final_Model/Normal_Image_Testing'\n",
        "\n",
        "sum_of_percentage = 0\n",
        "num_of_files = 0\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "# Iterate through all files in the folder\n",
        "for filename in os.listdir(dataset_folder):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):  # Check if it's an image file\n",
        "        # Load the image using PIL from the dataset folder\n",
        "        input_image = Image.open(os.path.join(dataset_folder, filename))\n",
        "        # Detect faces using the detect_faces method\n",
        "        bounding_boxes, landmarks = face_recongniser.detect_faces(input_image)\n",
        "\n",
        "        aligned_img = align_face(os.path.join(dataset_folder, filename), shape_predictor)[0]\n",
        "        img = transform(aligned_img).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            output = model(img.cuda())\n",
        "        out = util.tensor2im(output.data[0])\n",
        "\n",
        "        sum_of_percentage += get_similarity_percentage_using_torch(img, out)\n",
        "        num_of_files += 1\n",
        "\n",
        "# Stop the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the time taken\n",
        "elapsed_time = end_time - start_time\n",
        "elapsed_minutes = int(elapsed_time_seconds // 60)\n",
        "elapsed_seconds = int(elapsed_time_seconds % 60)\n",
        "\n",
        "average = sum_of_percentage / num_of_files\n",
        "print(f\"Average Similarity Percentage: {average:.2f}%\")\n",
        "print(f\"Time taken: {elapsed_minutes} minutes and {elapsed_seconds} seconds of {num_of_files} files\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
